##### connect with kaggle & download ######
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download  --force mohamedhanyyy/chest-ctscan-images

##### import  necessary library ######

import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import os
from skimage import io
from skimage.color import rgb2gray
import plotly.express as px
import random
from sklearn.utils import shuffle
import shutil
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications.resnet import preprocess_input
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import optimizers
from keras.applications import resnet

##### folder path ######

train_path = "/content/Data/train"
valid_path = "/content/Data/valid"
test_path = "/content/Data/test"

###########

from zipfile import ZipFile
file_name = "/content/chest-ctscan-images.zip"
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Done')

##### pre-processed applying clahe ######

import cv2
import os

def apply_clahe(image):
    clip_limit = 2.5
    tile_size = (8, 8)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)
    return clahe.apply(image)

train_dir = "/content/Data/train"
test_dir = "/content/Data/test"
valid_dir = "/content/Data/valid"

train_processed_dir = "/content/Data/train_processed"
test_processed_dir = "/content/Data/test_processed"
valid_processed_dir = "/content/Data/valid_processed"

os.makedirs(train_processed_dir, exist_ok=True)
os.makedirs(test_processed_dir, exist_ok=True)
os.makedirs(valid_processed_dir, exist_ok=True)

for subdir in os.listdir(train_dir):
    subdir_path = os.path.join(train_dir, subdir)
    if os.path.isdir(subdir_path):
        print(f'Applying CLAHE to images in {subdir_path}...')
        for filename in os.listdir(subdir_path):
            file_path = os.path.join(subdir_path, filename)
            if filename.endswith('.png'):
                image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
                clahe_image = apply_clahe(image)
                processed_file_path = os.path.join(train_processed_dir, subdir, filename)
                os.makedirs(os.path.dirname(processed_file_path), exist_ok=True)
                cv2.imwrite(processed_file_path, clahe_image)

for subdir in os.listdir(test_dir):
    subdir_path = os.path.join(test_dir, subdir)
    if os.path.isdir(subdir_path):
        print(f'Applying CLAHE to images in {subdir_path}...')
        for filename in os.listdir(subdir_path):
            file_path = os.path.join(subdir_path, filename)
            if filename.endswith('.png'):
                image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
                clahe_image = apply_clahe(image)
                processed_file_path = os.path.join(test_processed_dir, subdir, filename)
                os.makedirs(os.path.dirname(processed_file_path), exist_ok=True)
                cv2.imwrite(processed_file_path, clahe_image)

for subdir in os.listdir(valid_dir):
    subdir_path = os.path.join(valid_dir, subdir)
    if os.path.isdir(subdir_path):
        print(f'Applying CLAHE to images in {subdir_path}...')
        for filename in os.listdir(subdir_path):
            file_path = os.path.join(subdir_path, filename)
            if filename.endswith('.png'):
                image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)
                clahe_image = apply_clahe(image)
                processed_file_path = os.path.join(valid_processed_dir, subdir, filename)
                os.makedirs(os.path.dirname(processed_file_path), exist_ok=True)
                cv2.imwrite(processed_file_path, clahe_image)


#####  merging in one folder  ######

from tqdm._tqdm_notebook import tqdm_notebook as tqdm
import os,glob
import cv2

import os
import cv2
from tqdm import tqdm

# train_folder = train_path
# valid_folder = valid_path
# test_folder = test_path

train_folder = train_processed_dir
valid_folder = valid_processed_dir
test_folder = test_processed_dir

X = []
y = []

for folder_name in os.listdir(train_folder):
    folder_path = os.path.join(train_folder, folder_name)
    if os.path.isdir(folder_path):
        for i in tqdm(os.listdir(folder_path)):
            img_path = os.path.join(folder_path, i)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))
            X.append(img)
            y.append(folder_name[0])

for folder_name in os.listdir(valid_folder):
    folder_path = os.path.join(valid_folder, folder_name)
    if os.path.isdir(folder_path):
        for i in tqdm(os.listdir(folder_path)):
            img_path = os.path.join(folder_path, i)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))
            X.append(img)
            y.append(folder_name[0])

for folder_name in os.listdir(test_folder):
    folder_path = os.path.join(test_folder, folder_name)
    if os.path.isdir(folder_path):
        for i in tqdm(os.listdir(folder_path)):
            img_path = os.path.join(folder_path, i)
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))
            X.append(img)
            y.append(folder_name[0])


##### labelling and train-test splitting ######

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print ("Shape of an image in X_train: ", X_train[0].shape)
print ("Shape of an image in X_test: ", X_test[0].shape)
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.fit_transform(y_test)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=4)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=4)
y_train = np.array(y_train)
X_train = np.array(X_train)
y_test = np.array(y_test)
X_test = np.array(X_test)
print("X_train Shape: ", X_train.shape)
print("X_test Shape: ", X_test.shape)
print("y_train Shape: ", y_train.shape)
print("y_test Shape: ", y_test.shape)



##### cnn-model #####
model = tf.keras.Sequential()
#1st conv layer
model.add(tf.keras.layers.Conv2D(32, 3, input_shape=(224, 224, 3)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#2nd conv layer
model.add(tf.keras.layers.Conv2D(64, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#3rd conv layer (from here model gives good result)
model.add(tf.keras.layers.Conv2D(128, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#4th conv layer
model.add(tf.keras.layers.Conv2D(256, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#Flatten Layer
model.add(tf.keras.layers.Flatten())
#Dense Layer 1
model.add(tf.keras.layers.Dense(8000))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(1024))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(512))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
#Dense Layer 2
model.add(tf.keras.layers.Dense(100,name ='feature_denseee')) #100 Prominant Features are Extracted From This Layer
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
#output Dense Layer
model.add(tf.keras.layers.Dense(4))
model.add(tf.keras.layers.Activation('softmax'))
adam = tf.keras.optimizers.Adam(lr=0.001)

model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')
model.summary()

history = model.fit(X_train, y_train, batch_size= 32, epochs=100, verbose=1, validation_data=(X_test, y_test))

plt.plot(history.history['acc'], label = 'train',)
plt.plot(history.history['val_acc'], label = 'valid')

plt.legend(loc = 'upper left')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()

plt.plot(history.history['loss'], label = 'train',)
plt.plot(history.history['val_loss'], label = 'valid')

plt.legend(loc = 'upper left')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.show()

##### save the model , model loading and feature saving #####

model.save('/content/Data/model/lungcancer.h5')
model = tf.keras.models.load_model('/content/Data/model/lungcancer.h5')
intermediate_layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('feature_denseee').output)
intermediate_layer_model.summary()

X = np.array(X)
feature_engg_data = intermediate_layer_model.predict(X)
feature_engg_data = pd.DataFrame(feature_engg_data)
feature_engg_data.to_pickle('/content/Data/model/finalfeaturescovid.pkl')
features = pd.read_pickle('/content/Data/model/finalfeaturescovid.pkl')
from sklearn.preprocessing import StandardScaler
x = feature_engg_data.loc[:, feature_engg_data.columns].values
x = StandardScaler().fit_transform(x)
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, stratify= y, random_state =0)

##### svd  #####
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=100)
training_feature_svd_2D = svd.fit_transform(X_train)
test_feature_svd_2D = svd.transform(X_test)

'''GNB'''
from sklearn.model_selection import cross_val_score, KFold
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.naive_bayes import GaussianNB
from sklearn import  metrics
model = GaussianNB()
model.fit(training_feature_svd_2D, y_train)
y_pred1 = model.predict(test_feature_svd_2D)
metrics.accuracy_score(y_test, y_pred1)
print(metrics.accuracy_score(y_test, y_pred1))
print(metrics.classification_report(y_test, y_pred1))
y_pred_prob = model.predict_proba(test_feature_svd_2D)
auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')
print("AUC:", auc)

import matplotlib.pyplot as plt
import seaborn as sns
confusion = confusion_matrix(y_test, y_pred1)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

''roc curve for gnb''
# Compute ROC curve for each class
n_classes = len(np.unique(y_test))
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

fpr = dict()
tpr = dict()
roc_auc_scores = dict()  
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])
    roc_auc_scores[i] = roc_auc_score(y_test_bin[:, i], y_pred_prob[:, i])  
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_prob.ravel())
roc_auc_scores["micro"] = roc_auc_score(y_test_bin, y_pred_prob, average='micro')  

plt.figure(figsize=(8, 6))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = {:.2f}) for class {}'.format(roc_auc_scores[i], i))

plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (area = {:.2f})'.format(roc_auc_scores["micro"]), linestyle=':', linewidth=4)

plt.plot([0, 1], [0, 1], 'k--')  
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()


''''GBM'''
from sklearn.ensemble import GradientBoostingClassifier
import sklearn.metrics as metrics
gbm_classifier = GradientBoostingClassifier()
gbm_classifier.fit(training_feature_svd_2D, y_train)
y_pred_gbm = gbm_classifier.predict(test_feature_svd_2D)
accuracy = metrics.accuracy_score(y_test, y_pred_gbm)
print("Accuracy:", accuracy)
classification_report = metrics.classification_report(y_test, y_pred_gbm)
y_pred_prob_gbm = gbm_classifier.predict_proba(test_feature_svd_2D)
classification_report = metrics.classification_report(y_test, y_pred_gbm)
print("Classification Report:\n", classification_report)
auc = metrics.roc_auc_score(y_test, y_pred_prob_gbm, multi_class='ovr')
print("AUC:", auc)
import matplotlib.pyplot as plt
import seaborn as sns
confusion_matrix_gbm = metrics.confusion_matrix(y_test, y_pred_gbm)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix_gbm, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix - GBM')
plt.show()

######roc curve for gbm#####
# Compute ROC curve for each class
n_classes = len(np.unique(y_test))
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

fpr = dict()
tpr = dict()
roc_auc_scores = dict()  # Renamed the dictionary variable
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob_gbm[:, i])
    roc_auc_scores[i] = roc_auc_score(y_test_bin[:, i], y_pred_prob_gbm[:, i])  

fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_prob_gbm.ravel())
roc_auc_scores["micro"] = roc_auc_score(y_test_bin, y_pred_prob_gbm, average='micro')  
plt.figure(figsize=(8, 6))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = {:.2f}) for class {}'.format(roc_auc_scores[i], i))

plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (area = {:.2f})'.format(roc_auc_scores["micro"]), linestyle=':', linewidth=4)

plt.plot([0, 1], [0, 1], 'k--')  # Plot the random guessing curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()


'''SVM'''

from sklearn.svm import SVC
svclassifier = SVC(kernel='sigmoid', probability=True)
svclassifier.fit(training_feature_svd_2D , y_train)
y_pred2 = svclassifier.predict(test_feature_svd_2D)
metrics.accuracy_score(y_test, y_pred2)
print(metrics.classification_report(y_test, y_pred2))
print(metrics.accuracy_score(y_test, y_pred2))
y_pred_prob_svm = svclassifier.predict_proba(test_feature_svd_2D)
auc = roc_auc_score(y_test, y_pred_prob_svm, multi_class='ovr')
print("AUC:", auc)
import matplotlib.pyplot as plt
import seaborn as sns
confusion = confusion_matrix(y_test, y_pred2)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

#### roc curve for svm #####

n_classes = len(np.unique(y_test))
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
fpr = dict()
tpr = dict()
roc_auc_scores = dict()  
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob_svm[:, i])
    roc_auc_scores[i] = roc_auc_score(y_test_bin[:, i], y_pred_prob_svm[:, i])  
# Compute micro-average ROC curve and AUC
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_prob_svm.ravel())
roc_auc_scores["micro"] = roc_auc_score(y_test_bin, y_pred_prob_svm, average='micro')  

plt.figure(figsize=(8, 6))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = {:.2f}) for class {}'.format(roc_auc_scores[i], i))
plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (area = {:.2f})'.format(roc_auc_scores["micro"]), linestyle=':', linewidth=4)
plt.plot([0, 1], [0, 1], 'k--') 
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()


''''KNN'''' 

from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
knn = KNeighborsClassifier()
knn.fit(training_feature_svd_2D, y_train)
y_pred3 = knn.predict(test_feature_svd_2D)
accuracy = metrics.accuracy_score(y_test, y_pred3)
print("Accuracy:", accuracy)
classification_report = metrics.classification_report(y_test, y_pred3)
print("Classification Report:\n", classification_report)
y_pred_prob_knn = knn.predict_proba(test_feature_svd_2D)
auc = metrics.roc_auc_score(y_test, y_pred_prob_knn, multi_class='ovr')
print("AUC:", auc)
confusion = metrics.confusion_matrix(y_test, y_pred3)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

### roc for knn ####

n_classes = len(np.unique(y_test))
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

fpr = dict()
tpr = dict()
roc_auc_scores = dict()  
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob_knn[:, i])
    roc_auc_scores[i] = roc_auc_score(y_test_bin[:, i], y_pred_prob_knn[:, i])  
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_prob_knn.ravel())
roc_auc_scores["micro"] = roc_auc_score(y_test_bin, y_pred_prob_knn, average='micro')  

plt.figure(figsize=(8, 6))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = {:.2f}) for class {}'.format(roc_auc_scores[i], i))

plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (area = {:.2f})'.format(roc_auc_scores["micro"]), linestyle=':', linewidth=4)

plt.plot([0, 1], [0, 1], 'k--')  
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()



'''RF'''

from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier(n_estimators=50, random_state=1)
RF = RF.fit(training_feature_svd_2D, y_train)
y_pred5 = RF.predict(test_feature_svd_2D)
print(metrics.accuracy_score(y_test, y_pred5))
print(metrics.classification_report(y_test, y_pred5))
y_pred_prob_rf = RF.predict_proba(test_feature_svd_2D)

auc = roc_auc_score(y_test, y_pred_prob_rf, multi_class='ovr')
print("AUC:", auc)
import matplotlib.pyplot as plt
import seaborn as sns
confusion = confusion_matrix(y_test, y_pred5)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

#### roc for rf #####


# Compute ROC curve for each class
n_classes = len(np.unique(y_test))
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

fpr = dict()
tpr = dict()
roc_auc_scores = dict() 
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob_rf[:, i])
    roc_auc_scores[i] = roc_auc_score(y_test_bin[:, i], y_pred_prob_rf[:, i])  

fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_prob_rf.ravel())
roc_auc_scores["micro"] = roc_auc_score(y_test_bin, y_pred_prob_rf, average='micro')  

plt.figure(figsize=(8, 6))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = {:.2f}) for class {}'.format(roc_auc_scores[i], i))

plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (area = {:.2f})'.format(roc_auc_scores["micro"]), linestyle=':', linewidth=4)

plt.plot([0, 1], [0, 1], 'k--') 
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()




'''VOTING CLASSIFIER''''

from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.ensemble import VotingClassifier


voting_classifier = VotingClassifier(estimators=[('gbm_classifier', gbm_classifier), ('svm', svclassifier), ('knn', knn), ('GNB', model), ('rf', RF)], voting='hard')
voting_classifier.fit(training_feature_svd_2D, y_train)

y_pred_vot = voting_classifier.predict(test_feature_svd_2D)

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)
y_pred_encoded = label_encoder.transform(y_pred_vot)

onehot_encoder = OneHotEncoder(sparse=False)
y_test_onehot = onehot_encoder.fit_transform(y_test_encoded.reshape(-1, 1))
y_pred_onehot = onehot_encoder.transform(y_pred_encoded.reshape(-1, 1))

accuracy = metrics.accuracy_score(y_test_encoded, y_pred_encoded)
report = classification_report(y_test_encoded, y_pred_encoded)
auc = roc_auc_score(y_test_onehot, y_pred_onehot, multi_class='ovo')

print("Accuracy:", accuracy)
print("Classification Report:\n", report)
print("AUC:", auc)

from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score


precision = precision_score(y_test_encoded, y_pred_encoded, average='macro')
recall = recall_score(y_test_encoded, y_pred_encoded, average='macro')
f1 = f1_score(y_test_encoded, y_pred_encoded, average='macro')

# Print precision, recall, F1-score
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

### roc for voting classifier #####

from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt


voting_classifier = VotingClassifier(estimators=[('gbm_classifier', gbm_classifier), ('svm', svclassifier), ('knn', knn), ('GNB', model), ('rf', RF)], voting='soft')
voting_classifier.fit(training_feature_svd_2D, y_train)

y_pred_vot_prob = voting_classifier.predict_proba(test_feature_svd_2D)

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test)

fpr = dict()
tpr = dict()
roc_auc_dict = dict() 
n_classes = len(label_encoder.classes_)
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_onehot[:, i], y_pred_vot_prob[:, i])
    roc_auc_dict[i] = auc(fpr[i], tpr[i]) 

fpr["micro"], tpr["micro"], _ = roc_curve(y_test_onehot.ravel(), y_pred_vot_prob.ravel())
roc_auc_dict["micro"] = auc(fpr["micro"], tpr["micro"])  

plt.figure(figsize=(8, 6))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = {:.2f}) for class {}'.format(roc_auc_dict[i], label_encoder.classes_[i]))

plt.plot(fpr["micro"], tpr["micro"], label='Micro-average ROC curve (area = {:.2f})'.format(roc_auc_dict["micro"]), linestyle=':', linewidth=4)

plt.plot([0, 1], [0, 1], 'k--')  
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()



*************************************binary ***********************


from tqdm import tqdm
import os
import cv2
import numpy as np

train_folder = train_processed_dir
valid_folder = valid_processed_dir
test_folder = test_processed_dir

X = []
y = []

class_labels = {'cancer': ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa'],
                'non-cancer': ['normal']}

for class_label, subfolders in class_labels.items():
    for subfolder in subfolders:
        folder_path = os.path.join(train_folder, subfolder)
        if os.path.isdir(folder_path):
            for filename in tqdm(os.listdir(folder_path)):
                img_path = os.path.join(folder_path, filename)
                img = cv2.imread(img_path)
                img = cv2.resize(img, (224, 224))
                X.append(img)
                y.append(class_label)

class_labels2 = {'cancer': ['adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib', 'large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa', 'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa'],
                'non-cancer': ['normal']}
for class_label, subfolders in class_labels2.items():
    for subfolder in subfolders:
        folder_path = os.path.join(valid_folder, subfolder)
        if os.path.isdir(folder_path):
            for filename in tqdm(os.listdir(folder_path)):
                img_path = os.path.join(folder_path, filename)
                img = cv2.imread(img_path)
                img = cv2.resize(img, (224, 224))
                X.append(img)
                y.append(class_label)


class_labels3 = {'cancer': ['adenocarcinoma', 'large.cell.carcinoma', 'squamous.cell.carcinoma'],
                'non-cancer': ['normal']}
for class_label, subfolders in class_labels3.items():
    for subfolder in subfolders:
        folder_path = os.path.join(test_folder, subfolder)
        if os.path.isdir(folder_path):
            for filename in tqdm(os.listdir(folder_path)):
                img_path = os.path.join(folder_path, filename)
                img = cv2.imread(img_path)
                img = cv2.resize(img, (224, 224))
                X.append(img)
                y.append(class_label)

y_binary = np.array([0 if label == 'non-cancer' else 1 for label in y])



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print ("Shape of an image in X_train: ", X_train[0].shape)
print ("Shape of an image in X_test: ", X_test[0].shape)
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.fit_transform(y_test)
y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)
y_train = np.array(y_train)
X_train = np.array(X_train)
y_test = np.array(y_test)
X_test = np.array(X_test)
print("X_train Shape: ", X_train.shape)
print("X_test Shape: ", X_test.shape)
print("y_train Shape: ", y_train.shape)
print("y_test Shape: ", y_test.shape)


model = tf.keras.Sequential()
#1st conv layer
model.add(tf.keras.layers.Conv2D(32, 3, input_shape=(224, 224, 3)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#2nd conv layer
model.add(tf.keras.layers.Conv2D(64, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#3rd conv layer (from here model gives good result)
model.add(tf.keras.layers.Conv2D(128, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#4th conv layer
model.add(tf.keras.layers.Conv2D(256, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
#Flatten Layer
model.add(tf.keras.layers.Flatten())
#Dense Layer 1
model.add(tf.keras.layers.Dense(8000))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(1024))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(512))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
#Dense Layer 2
model.add(tf.keras.layers.Dense(100,name ='feature_denseee')) #100 Prominant Features are Extracted From This Layer
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
#output Dense Layer
model.add(tf.keras.layers.Dense(2))
model.add(tf.keras.layers.Activation('softmax'))
adam = tf.keras.optimizers.Adam(lr=0.001)

model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')
model.summary()


history = model.fit(X_train, y_train, batch_size= 32, epochs=100, verbose=1, validation_data=(X_test, y_test))


plt.plot(history.history['acc'], label = 'train',)
plt.plot(history.history['val_acc'], label = 'valid')

plt.legend(loc = 'upper left')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()
plt.plot(history.history['loss'], label = 'train',)
plt.plot(history.history['val_loss'], label = 'valid')

plt.legend(loc = 'upper left')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.show()

model.save('/content/Data/model/lcancer.h5')
model = tf.keras.models.load_model('/content/Data/model/lcancer.h5')
intermediate_layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('dense_2').output)
intermediate_layer_model.summary()
X = np.array(X)
feature_engg_data = intermediate_layer_model.predict(X)
feature_engg_data = pd.DataFrame(feature_engg_data)
feature_engg_data.to_pickle('/content/Data/model/finalfeaturescovid.pkl')
features = pd.read_pickle('/content/Data/model/finalfeaturescovid.pkl')
from sklearn.preprocessing import StandardScaler
x = feature_engg_data.loc[:, feature_engg_data.columns].values
x = StandardScaler().fit_transform(x)
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, stratify= y, random_state =0)
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=100)
training_feature_svd_2D = svd.fit_transform(X_train)
test_feature_svd_2D = svd.transform(X_test)

****************gnb 
from sklearn.model_selection import cross_val_score, KFold
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.naive_bayes import GaussianNB
from sklearn import  metrics
model = GaussianNB()
model.fit(training_feature_svd_2D, y_train)
y_pred1 = model.predict(test_feature_svd_2D)
metrics.accuracy_score(y_test, y_pred1)
print(metrics.accuracy_score(y_test, y_pred1))
print(metrics.classification_report(y_test, y_pred1))
y_pred_prob = model.predict_proba(test_feature_svd_2D)

confusion = confusion_matrix(y_test, y_pred1)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

************** svm

from sklearn.svm import SVC

svm_model = SVC(probability=True)
svm_model.fit(training_feature_svd_2D, y_train)
y_pred_svm = svm_model.predict(test_feature_svd_2D)

svm_accuracy = metrics.accuracy_score(y_test, y_pred_svm)
print("SVM Accuracy:", svm_accuracy)

svm_classification_report = metrics.classification_report(y_test, y_pred_svm)
print("SVM Classification Report:\n", svm_classification_report)
y_pred_prob_svm = svm_model.predict_proba(test_feature_svd_2D)
svm_auc = roc_auc_score(y_test, y_pred_prob_svm[:, 1])
print("SVM AUC:", svm_auc)


from sklearn.svm import SVC
from sklearn.metrics import roc_curve, auc, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

svm_model = SVC(probability=True)
svm_model.fit(training_feature_svd_2D, y_train)

from sklearn.preprocessing import LabelBinarizer

label_binarizer = LabelBinarizer()
y_test_binary = label_binarizer.fit_transform(y_test)

y_pred_prob_svm = svm_model.predict_proba(test_feature_svd_2D)[:, 1]

fpr_svm, tpr_svm, _ = roc_curve(y_test_binary, y_pred_prob_svm)
roc_auc_svm = auc(fpr_svm, tpr_svm)

plt.figure(figsize=(8, 6))
plt.plot(fpr_svm, tpr_svm, color='darkorange', lw=2, label='SVM ROC curve (AUC = %0.2f)' % roc_auc_svm)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

confusion_svm = confusion_matrix(y_test, y_pred_svm)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_svm, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix (SVM)')
plt.show()


************* roc and cm for gnb

from sklearn.naive_bayes import GaussianNB

gnb_model = GaussianNB()
gnb_model.fit(training_feature_svd_2D, y_train)

y_pred_prob_gnb = gnb_model.predict_proba(test_feature_svd_2D)[:, 1]

fpr_gnb, tpr_gnb, _ = roc_curve(y_test_binary, y_pred_prob_gnb)
roc_auc_gnb = auc(fpr_gnb, tpr_gnb)

plt.figure(figsize=(8, 6))
plt.plot(fpr_gnb, tpr_gnb, color='green', lw=2, label='GNB ROC curve (AUC = %0.2f)' % roc_auc_gnb)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (GNB)')
plt.legend(loc='lower right')
plt.show()

y_pred_gnb = gnb_model.predict(test_feature_svd_2D)
confusion_gnb = confusion_matrix(y_test, y_pred_gnb)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_gnb, annot=True, cmap='Blues', fmt='d')

plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix (GNB)')
plt.show()

*********** roc and cm for gbm 

from sklearn.ensemble import GradientBoostingClassifier

gbm_model = GradientBoostingClassifier()
gbm_model.fit(training_feature_svd_2D, y_train)

y_pred_prob_gbm = gbm_model.predict_proba(test_feature_svd_2D)[:, 1]

fpr_gbm, tpr_gbm, _ = roc_curve(y_test_binary, y_pred_prob_gbm)
roc_auc_gbm = auc(fpr_gbm, tpr_gbm)

plt.figure(figsize=(8, 6))
plt.plot(fpr_gbm, tpr_gbm, color='blue', lw=2, label='GBM ROC curve (AUC = %0.2f)' % roc_auc_gbm)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (GBM)')
plt.legend(loc='lower right')
plt.show()

y_pred_gbm = gbm_model.predict(test_feature_svd_2D)
confusion_gbm = confusion_matrix(y_test, y_pred_gbm)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_gbm, annot=True, cmap='Blues', fmt='d')

plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix (GBM)')
plt.show()

*************** roc and cm for knn 

from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier()
knn_model.fit(training_feature_svd_2D, y_train)

y_pred_prob_knn = knn_model.predict_proba(test_feature_svd_2D)[:, 1]

fpr_knn, tpr_knn, _ = roc_curve(y_test_binary, y_pred_prob_knn)
roc_auc_knn = auc(fpr_knn, tpr_knn)

plt.figure(figsize=(8, 6))
plt.plot(fpr_knn, tpr_knn, color='purple', lw=2, label='KNN ROC curve (AUC = %0.2f)' % roc_auc_knn)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (KNN)')
plt.legend(loc='lower right')
plt.show()

y_pred_knn = knn_model.predict(test_feature_svd_2D)
confusion_knn = confusion_matrix(y_test, y_pred_knn)

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_knn, annot=True, cmap='Blues', fmt='d')

plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix (KNN)')
plt.show()

************** roc and cm for rf 

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier()
rf_model.fit(training_feature_svd_2D, y_train)

y_pred_prob_rf = rf_model.predict_proba(test_feature_svd_2D)[:, 1]

fpr_rf, tpr_rf, _ = roc_curve(y_test_binary, y_pred_prob_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='red', lw=2, label='RF ROC curve (AUC = %0.2f)' % roc_auc_rf)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (RF)')
plt.legend(loc='lower right')
plt.show()

y_pred_rf = rf_model.predict(test_feature_svd_2D)
confusion_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_rf, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix (RF)')
plt.show()

************ voting classifier roc and cm

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import roc_curve, auc, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

gnb_model = GaussianNB()
gbm_model = GradientBoostingClassifier()
knn_model = KNeighborsClassifier()
svm_model = SVC(probability=True)
rf_model = RandomForestClassifier()

voting_classifier = VotingClassifier(estimators=[
    ('GNB', gnb_model),
    ('GBM', gbm_model),
    ('KNN', knn_model),
    ('SVM', svm_model),
    ('RF', rf_model)
], voting='soft')  # Use soft voting for probabilities

voting_classifier.fit(training_feature_svd_2D, y_train)

y_pred_prob_voting = voting_classifier.predict_proba(test_feature_svd_2D)[:, 1]

fpr_voting, tpr_voting, _ = roc_curve(y_test_binary, y_pred_prob_voting)
roc_auc_voting = auc(fpr_voting, tpr_voting)

plt.figure(figsize=(8, 6))
plt.plot(fpr_voting, tpr_voting, color='purple', lw=2, label='Voting ROC curve (AUC = %0.2f)' % roc_auc_voting)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Voting)')
plt.legend(loc='lower right')
plt.show()

y_pred_voting = voting_classifier.predict(test_feature_svd_2D)
confusion_voting = confusion_matrix(y_test, y_pred_voting)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_voting, annot=True, cmap='Blues', fmt='d')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix (Voting)')
plt.show()

**********voting classifier classification result

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

gnb_classifier = GaussianNB()
gbm_classifier = GradientBoostingClassifier()
knn_classifier = KNeighborsClassifier()
svm_classifier = SVC(probability=True)
rf_classifier = RandomForestClassifier()

voting_classifier = VotingClassifier(estimators=[
    ('GNB', gnb_classifier),
    ('GBM', gbm_classifier),
    ('KNN', knn_classifier),
    ('SVM', svm_classifier),
    ('RF', rf_classifier)
], voting='soft')  # Use soft voting for probabilities

voting_classifier.fit(training_feature_svd_2D, y_train)
y_pred_voting = voting_classifier.predict(test_feature_svd_2D)
voting_accuracy = accuracy_score(y_test, y_pred_voting)
print("Voting Classifier Accuracy:", voting_accuracy)
voting_classification_report = classification_report(y_test, y_pred_voting)
print("Voting Classifier Classification Report:\n", voting_classification_report)
y_pred_prob_voting = voting_classifier.predict_proba(test_feature_svd_2D)
voting_auc = roc_auc_score(y_test, y_pred_prob_voting[:, 1])
print("Voting Classifier AUC:", voting_auc)

***********gbm

from sklearn.ensemble import GradientBoostingClassifier
gbm_model = GradientBoostingClassifier()
gbm_model.fit(training_feature_svd_2D, y_train)
y_pred_gbm = gbm_model.predict(test_feature_svd_2D)
gbm_accuracy = metrics.accuracy_score(y_test, y_pred_gbm)
print("GBM Accuracy:", gbm_accuracy)
gbm_classification_report = metrics.classification_report(y_test, y_pred_gbm)
print("GBM Classification Report:\n", gbm_classification_report)
y_pred_prob_gbm = gbm_model.predict_proba(test_feature_svd_2D)
gbm_auc = roc_auc_score(y_test, y_pred_prob_gbm[:, 1])
print("GBM AUC:", gbm_auc)

*************knn

from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier()
knn_model.fit(training_feature_svd_2D, y_train)
y_pred_knn = knn_model.predict(test_feature_svd_2D)
knn_accuracy = metrics.accuracy_score(y_test, y_pred_knn)
print("KNN Accuracy:", knn_accuracy)
knn_classification_report = metrics.classification_report(y_test, y_pred_knn)
print("KNN Classification Report:\n", knn_classification_report)
y_pred_prob_knn = knn_model.predict_proba(test_feature_svd_2D)
knn_auc = roc_auc_score(y_test, y_pred_prob_knn[:, 1])
print("KNN AUC:", knn_auc)


************** rf
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier()
rf_model.fit(training_feature_svd_2D, y_train)
y_pred_rf = rf_model.predict(test_feature_svd_2D)
rf_accuracy = metrics.accuracy_score(y_test, y_pred_rf)
print("RF Accuracy:", rf_accuracy)
rf_classification_report = metrics.classification_report(y_test, y_pred_rf)
print("RF Classification Report:\n", rf_classification_report)
y_pred_prob_rf = rf_model.predict_proba(test_feature_svd_2D)
rf_auc = roc_auc_score(y_test, y_pred_prob_rf[:, 1])
print("RF AUC:", rf_auc)
