import pandas as pd
import numpy as np
import os
import cv2
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import (roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, 
                             classification_report, accuracy_score)
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Test for GPU or CPU devices
print("Device: \n", tf.config.experimental.list_physical_devices())
print(tf.__version__)
print(tf.test.is_built_with_cuda())

# CLAHE Function for preprocessing
def clahe_function(img):
    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))
    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab_img)
    clahe_img = clahe.apply(l)
    updated_lab_img2 = cv2.merge((clahe_img, a, b))
    CLAHE_img = cv2.cvtColor(updated_lab_img2, cv2.COLOR_LAB2BGR)
    return CLAHE_img

# Load images function
def load_images_from_folder(folder_path):
    images = []
    labels = []
    class_folders = os.listdir(folder_path)
    
    for label, class_folder in enumerate(class_folders):
        class_folder_path = os.path.join(folder_path, class_folder)
        for filename in os.listdir(class_folder_path):
            img_path = os.path.join(class_folder_path, filename)
            img = cv2.imread(img_path)
            img = clahe_function(img)
            img = cv2.resize(img, (224, 224))
            img = img / 255.0
            if img is not None:
                images.append(img)
                labels.append(label)
    return np.array(images), np.array(labels)

# Load datasets
train_folder = r"E:/sabbir/train"
test_folder = r"E:/sabbir/test"
valid_folder = r"E:/sabbir/valid"

X_train, y_train = load_images_from_folder(train_folder)
X_test, y_test = load_images_from_folder(test_folder)
X_valid, y_valid = load_images_from_folder(valid_folder)

# Merge all datasets
X_all = np.concatenate((X_train, X_test, X_valid), axis=0)
y_all = np.concatenate((y_train, y_test, y_valid), axis=0)

# Split the merged dataset
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(
    X_all, y_all, test_size=0.20, stratify=y_all, random_state=0
)

# Define and compile CNN model
model = tf.keras.Sequential()
# 1st conv layer
model.add(tf.keras.layers.Conv2D(32, 3, input_shape=(224, 224, 3)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
# 2nd conv layer
model.add(tf.keras.layers.Conv2D(64, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
# 3rd conv layer
model.add(tf.keras.layers.Conv2D(128, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
# 4th conv layer
model.add(tf.keras.layers.Conv2D(256, 3, padding="valid"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.MaxPooling2D(2))
# Flatten Layer
model.add(tf.keras.layers.Flatten())
# Dense Layer 1
model.add(tf.keras.layers.Dense(8000))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(1024))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(512, name='feature_denseee'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
# Dense Layer 2
model.add(tf.keras.layers.Dense(100))  
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation("relu"))
model.add(tf.keras.layers.Dropout(0.5))
# Output Dense Layer (Binary Classification)
model.add(tf.keras.layers.Dense(1))  # 1 neuron for binary classification
model.add(tf.keras.layers.Activation('sigmoid'))

adam = tf.keras.optimizers.Adam(learning_rate=0.001)

model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer=adam)
model.summary()

# Train the CNN model
history = model.fit(
    X_train_split, y_train_split, batch_size=32, epochs=100, verbose=1,
    validation_data=(X_test_split, y_test_split)
)

# Save and reload the model
model.save(r'E:/model2/save_binary.h5')
model = tf.keras.models.load_model(r'E:/model2/save_binary.h5')

# Create intermediate model for feature extraction
intermediate_layer_model = tf.keras.Model(
    inputs=model.input, outputs=model.get_layer('feature_denseee').output
)
intermediate_layer_model.summary()

# Feature extraction
feature_engg_data = intermediate_layer_model.predict(X_train_split)
feature_engg_data = pd.DataFrame(feature_engg_data)

# Save and load the features
feature_engg_data.to_pickle(r'E:/model2/finalfeatures_binary.pkl')
features = pd.read_pickle(r'E:/model2/finalfeatures_binary.pkl')

# Normalize features
x = feature_engg_data.values
x = StandardScaler().fit_transform(x)

# Apply SVD for dimensionality reduction
svd = TruncatedSVD(n_components=50)
x_reduced = svd.fit_transform(x)

# Split the reduced features into Training & Testing Set
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(
    x_reduced, y_train_split, test_size=0.20, stratify=y_train_split, random_state=0
)

# Apply Machine Learning Algorithms for Classification
# Gaussian Naive Bayes
model_gnb = GaussianNB()
model_gnb.fit(X_train_split, y_train_split)
y_pred_gnb = model_gnb.predict(X_test_split)

# Support Vector Machine
svclassifier = SVC(kernel='sigmoid', probability=True)
svclassifier.fit(X_train_split, y_train_split)
y_pred_svc = svclassifier.predict(X_test_split)

# Gradient Boosting Machine
model_gbm = GradientBoostingClassifier(n_estimators=100, random_state=1)
model_gbm.fit(X_train_split, y_train_split)
y_pred_gbm = model_gbm.predict(X_test_split)

# k-Nearest Neighbors
model_knn = KNeighborsClassifier(n_neighbors=5)
model_knn.fit(X_train_split, y_train_split)
y_pred_knn = model_knn.predict(X_test_split)

# Random Forest Classifier
RF = RandomForestClassifier(n_estimators=50, random_state=1)
RF.fit(X_train_split, y_train_split)
y_pred_rf = RF.predict(X_test_split)

# Ensemble Model
voting_classifier = VotingClassifier(
    estimators=[('GNB', model_gnb), ('SVM', svclassifier), ('GBM', model_gbm), ('KNN', model_knn), ('RF', RF)], 
    voting='soft'
)
voting_classifier.fit(X_train_split, y_train_split)
y_pred_vot = voting_classifier.predict(X_test_split)

# Compute metrics for each model
for model_name, y_pred in {
    "GNB": y_pred_gnb,
    "SVM": y_pred_svc,
    "GBM": y_pred_gbm,
    "KNN": y_pred_knn,
    "RF": y_pred_rf,
    "Ensemble": y_pred_vot
}.items():
    print(f"\nMetrics for {model_name}:")

    # Confusion Matrix
    cm = confusion_matrix(y_test_split, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix - {model_name}')
    plt.show()

    # Classification Report
    report = classification_report(y_test_split, y_pred, target_names=['Class 0', 'Class 1'])
    print(report)

    # Accuracy Score
    accuracy = accuracy_score(y_test_split, y_pred)
    print(f'Accuracy - {model_name}: {accuracy:.4f}')

    # ROC-AUC Score
    fpr, tpr, _ = roc_curve(y_test_split, y_pred)
    roc_auc = auc(fpr, tpr)
    print(f'ROC AUC - {model_name}: {roc_auc:.4f}')
